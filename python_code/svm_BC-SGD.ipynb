{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # for handling multi-dimensional array operation\n",
    "import pandas as pd  # for reading data from csv \n",
    "import statsmodels.api as sm  # for finding the p-value\n",
    "from sklearn.preprocessing import MinMaxScaler  # for normalization\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.utils import shuffle\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_correlated_features(X):\n",
    "#def remove_less_significant_features(X, Y):# >> MODEL TRAINING << #\n",
    "#def compute_cost(W, X, Y):\n",
    "#def calculate_cost_gradient(W, X_batch, Y_batch):\n",
    "#def sgd(features, outputs):def init():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(W, X, Y):\n",
    "    # calculate hinge loss\n",
    "    N = X.shape[0]\n",
    "    distances = 1 - Y * (np.dot(X, W))\n",
    "    distances[distances < 0] = 0  # equivalent to max(0, distance)\n",
    "    hinge_loss = reg_strength * (np.sum(distances) / N)\n",
    "    \n",
    "    # calculate cost\n",
    "    cost = 1 / 2 * np.dot(W, W) + hinge_loss\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost_gradient(W, X_batch, Y_batch):\n",
    "    # if only one example is passed (eg. in case of SGD)\n",
    "    if type(Y_batch) == np.float64:\n",
    "        Y_batch = np.array([Y_batch])\n",
    "        X_batch = np.array([X_batch])    \n",
    "    #print(Y_batch)\n",
    "    \n",
    "    distance = 1 - (Y_batch * np.dot(X_batch, W)) # np.dot(X_batch, W) is vector multiplication\n",
    "    dw = np.zeros(len(W))    \n",
    "    #print(distance)  \n",
    "    d = distance\n",
    "    if max(0, d) == 0:\n",
    "        di = W\n",
    "    else:\n",
    "        di = W - (reg_strength * Y_batch * X_batch)  \n",
    "    dw += di          \n",
    "    #dw = dw/len(Y_batch)  # average\n",
    "    return dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sgd(features, outputs):\n",
    "    max_epochs = 200\n",
    "    weights = np.zeros(features.shape[1])\n",
    "    nth = 0\n",
    "    prev_cost = float(\"inf\")\n",
    "    cost_threshold = 0.01  # in percent\n",
    "    # stochastic gradient descent\n",
    "    for epoch in range(1, max_epochs):\n",
    "        # shuffle to prevent repeating update cycles\n",
    "        X, Y = shuffle(features, outputs)\n",
    "        \n",
    "        # X_train has 455 rows. if epoch = 5000, the maximum amount of compute of gradient is 455 * 5000 = 2.5 M.\n",
    "        for ind, x in enumerate(X):\n",
    "            ascent = calculate_cost_gradient(weights, x, Y[ind]) #calculate one ascent for each row of X.\n",
    "            weights = weights - (learning_rate * ascent)       \n",
    "        \n",
    "        # convergence check on 2^nth epoch\n",
    "        if epoch == 2 ** nth or epoch == max_epochs - 1:\n",
    "            cost = compute_cost(weights, features, outputs)\n",
    "            #print(\"Epoch is:{} and Cost is: {}\".format(epoch, cost))\n",
    "            # stoppage criterion\n",
    "            if abs(prev_cost - cost) < cost_threshold * prev_cost:\n",
    "                return weights\n",
    "            prev_cost = cost\n",
    "            nth += 1\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> FEATURE SELECTION << #\n",
    "def remove_correlated_features(X):\n",
    "    corr_threshold = 0.9\n",
    "    corr = X.corr()\n",
    "    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= corr_threshold:\n",
    "                drop_columns[j] = True\n",
    "    columns_dropped = X.columns[drop_columns]\n",
    "    X.drop(columns_dropped, axis=1, inplace=True)\n",
    "    return columns_dropped\n",
    "\n",
    "def remove_less_significant_features(X, Y):\n",
    "    sl = 0.05\n",
    "    regression_ols = None\n",
    "    columns_dropped = np.array([])\n",
    "    for itr in range(0, len(X.columns)):\n",
    "        regression_ols = sm.OLS(Y, X).fit()\n",
    "        max_col = regression_ols.pvalues.idxmax()\n",
    "        max_val = regression_ols.pvalues.max()\n",
    "        if max_val > sl:\n",
    "            X.drop(max_col, axis='columns', inplace=True)\n",
    "            columns_dropped = np.append(columns_dropped, [max_col])\n",
    "        else:\n",
    "            break\n",
    "    regression_ols.summary()\n",
    "    return columns_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper-parameters and call init\n",
    "# hyper-parameters are normally tuned using cross-validation\n",
    "# but following work good enough\n",
    "reg_strength = 10000 # regularization strength\n",
    "learning_rate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')    # SVM only accepts numerical values. \n",
    "# Therefore, we will transform the categories M and B into values 1 and -1 (or -1 and 1), respectively.\n",
    "diagnosis_map = {'M':1, 'B':-1}\n",
    "data['diagnosis'] = data['diagnosis'].map(diagnosis_map)    # drop last column (extra column added by pd)\n",
    "    # and unnecessary first column (id)\n",
    "data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n",
    "Y = data.loc[:, 'diagnosis']  # all rows of 'diagnosis' \n",
    "X = data.iloc[:, 1:]  # all rows of column 1 and ahead (features)# normalize the features using MinMaxScalar from\n",
    "    # sklearn.preprocessing\n",
    "X_normalized = MinMaxScaler().fit_transform(X.values)\n",
    "X = pd.DataFrame(X_normalized)\n",
    "    # first insert 1 in every row for intercept b\n",
    "X.insert(loc=len(X.columns), column='intercept', value=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4., 25., 19., 18.,  8., 11., 15., 14.,  9.])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # filter features\n",
    "remove_correlated_features(X)\n",
    "remove_less_significant_features(X, Y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting dataset into train and test sets...\n"
     ]
    }
   ],
   "source": [
    "    # test_size is the portion of data that will go into test set\n",
    "    # random_state is the seed used by the random number generator\n",
    "print(\"splitting dataset into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(455, 12)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 12)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training started...\n",
      "training finished.\n",
      "weights are: [ 7.28445331  3.39542307 -0.87295575  3.67646092  4.43252477 -3.22779976\n",
      " -1.44377569  2.75109657  2.51752733  3.34388522 -0.89425965 -6.682658  ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# train the model\n",
    "print(\"training started...\")\n",
    "start = time.time()\n",
    "W = sgd(X_train.to_numpy(), y_train.to_numpy())\n",
    "end = time.time()\n",
    "print(\"training finished.\")\n",
    "print(\"weights are: {}\".format(W))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8471345901489258\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.012757062911987305\n",
      "accuracy on test dataset: 0.9736842105263158\n",
      "recall on test dataset: 0.9534883720930233\n",
      "precision on test dataset: 0.9534883720930233\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = np.array([])\n",
    "\n",
    "s = time.time()\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    yp = np.sign(np.dot(W, X_test.to_numpy()[i]))\n",
    "    y_test_predicted = np.append(y_test_predicted, yp)\n",
    "\n",
    "e = time.time()\n",
    "print(e-s)\n",
    "\n",
    "print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))\n",
    "print(\"recall on test dataset: {}\".format(recall_score(y_test.to_numpy(), y_test_predicted)))\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
