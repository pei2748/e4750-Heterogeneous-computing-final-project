{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  # for handling multi-dimensional array operation\n",
    "import pandas as pd  # for reading data from csv \n",
    "import statsmodels.api as sm  # for finding the p-value\n",
    "from sklearn.preprocessing import MinMaxScaler  # for normalization\n",
    "from sklearn.model_selection import train_test_split as tts\n",
    "from sklearn.metrics import accuracy_score, recall_score\n",
    "from sklearn.utils import shuffle\n",
    "import cv2 as cv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def remove_correlated_features(X):\n",
    "#def remove_less_significant_features(X, Y):# >> MODEL TRAINING << #\n",
    "#def compute_cost(W, X, Y):\n",
    "#def calculate_cost_gradient(W, X_batch, Y_batch):\n",
    "#def sgd(features, outputs):def init():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# >> FEATURE SELECTION << #\n",
    "def remove_correlated_features(X):\n",
    "    corr_threshold = 0.9\n",
    "    corr = X.corr()\n",
    "    drop_columns = np.full(corr.shape[0], False, dtype=bool)\n",
    "    for i in range(corr.shape[0]):\n",
    "        for j in range(i + 1, corr.shape[0]):\n",
    "            if corr.iloc[i, j] >= corr_threshold:\n",
    "                drop_columns[j] = True\n",
    "    columns_dropped = X.columns[drop_columns]\n",
    "    X.drop(columns_dropped, axis=1, inplace=True)\n",
    "    return columns_dropped\n",
    "\n",
    "def remove_less_significant_features(X, Y):\n",
    "    sl = 0.05\n",
    "    regression_ols = None\n",
    "    columns_dropped = np.array([])\n",
    "    for itr in range(0, len(X.columns)):\n",
    "        regression_ols = sm.OLS(Y, X).fit()\n",
    "        max_col = regression_ols.pvalues.idxmax()\n",
    "        max_val = regression_ols.pvalues.max()\n",
    "        if max_val > sl:\n",
    "            X.drop(max_col, axis='columns', inplace=True)\n",
    "            columns_dropped = np.append(columns_dropped, [max_col])\n",
    "        else:\n",
    "            break\n",
    "    regression_ols.summary()\n",
    "    return columns_dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set hyper-parameters and call init\n",
    "# hyper-parameters are normally tuned using cross-validation\n",
    "# but following work good enough\n",
    "reg_strength = 10000 # regularization strength\n",
    "learning_rate = 0.000001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv')    # SVM only accepts numerical values. \n",
    "# Therefore, we will transform the categories M and B into values 1 and -1 (or -1 and 1), respectively.\n",
    "diagnosis_map = {'M':1, 'B':-1}\n",
    "data['diagnosis'] = data['diagnosis'].map(diagnosis_map)    # drop last column (extra column added by pd)\n",
    "    # and unnecessary first column (id)\n",
    "data.drop(data.columns[[-1, 0]], axis=1, inplace=True)\n",
    "Y = data.loc[:, 'diagnosis']  # all rows of 'diagnosis' \n",
    "X = data.iloc[:, 1:]  # all rows of column 1 and ahead (features)# normalize the features using MinMaxScalar from\n",
    "    # sklearn.preprocessing\n",
    "X_normalized = MinMaxScaler().fit_transform(X.values)\n",
    "X = pd.DataFrame(X_normalized)\n",
    "    # first insert 1 in every row for intercept b\n",
    "X.insert(loc=len(X.columns), column='intercept', value=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4., 25., 19., 18.,  8., 11., 15., 14.,  9.])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    # filter features\n",
    "remove_correlated_features(X)\n",
    "remove_less_significant_features(X, Y)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "splitting dataset into train and test sets...\n"
     ]
    }
   ],
   "source": [
    "    # test_size is the portion of data that will go into test set\n",
    "    # random_state is the seed used by the random number generator\n",
    "print(\"splitting dataset into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = tts(X, Y, test_size=0.2, random_state=42)\n",
    "    \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.matrix(np.array(X_train).astype('float32'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.matrix(np.array(X_test).astype('float32'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = cv.ml.SVM_create()\n",
    "svm.setType(cv.ml.SVM_C_SVC)\n",
    "svm.setKernel(cv.ml.SVM_LINEAR)\n",
    "# 200 is the max_iter, 1e-6 is the stop criteria.\n",
    "svm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, 200, 1e-6))\n",
    "## [init]\n",
    "## [train]\n",
    "start = time.time()\n",
    "svm.train(X_train, cv.ml.ROW_SAMPLE, y_train)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0020601749420166016\n"
     ]
    }
   ],
   "source": [
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003133535385131836\n"
     ]
    }
   ],
   "source": [
    "y_test_predicted = np.array([])\n",
    "s = time.time()\n",
    "for i in range(X_test.shape[0]):\n",
    "    yp = svm.predict(X_test[i])\n",
    "    y_test_predicted = np.append(y_test_predicted, yp[1])\n",
    "\n",
    "e = time.time()\n",
    "\n",
    "print(e - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on test dataset: 0.9473684210526315\n",
      "recall on test dataset: 0.8837209302325582\n",
      "precision on test dataset: 0.8837209302325582\n"
     ]
    }
   ],
   "source": [
    "print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test.to_numpy(), y_test_predicted)))\n",
    "print(\"recall on test dataset: {}\".format(recall_score(y_test.to_numpy(), y_test_predicted)))\n",
    "print(\"precision on test dataset: {}\".format(recall_score(y_test.to_numpy(), y_test_predicted)))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n",
      "0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "t = acc = recall = []\n",
    "\n",
    "for m in range(100):\n",
    "    max_iter = (m+1) * 100\n",
    "    svm = cv.ml.SVM_create()\n",
    "    svm.setType(cv.ml.SVM_C_SVC)\n",
    "    svm.setKernel(cv.ml.SVM_LINEAR)\n",
    "    # 200 is the max_iter, 1e-6 is the stop criteria.\n",
    "    svm.setTermCriteria((cv.TERM_CRITERIA_MAX_ITER, max_iter, 1e-6))\n",
    "    ## [init]\n",
    "    ## [train]\n",
    "    start = time.time()\n",
    "    svm.train(X_train, cv.ml.ROW_SAMPLE, y_train)\n",
    "    end = time.time()\n",
    "    t.append(start - end)\n",
    "    \n",
    "    y_test_predicted = np.array([])\n",
    "    \n",
    "    for i in range(X_test.shape[0]):\n",
    "        yp = svm.predict(X_test[i])\n",
    "        y_test_predicted = np.append(y_test_predicted, yp[1])\n",
    "\n",
    "    a = accuracy_score(y_test.to_numpy(), y_test_predicted)\n",
    "    r = recall_score(y_test.to_numpy(), y_test_predicted)\n",
    "    print(a)\n",
    "    acc.append(a)\n",
    "    recall.append(r)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0012619495391845703,\n",
       " 0.9473684210526315,\n",
       " 0.9069767441860465,\n",
       " -0.0012812614440917969,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0008351802825927734,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.001252889633178711,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0011758804321289062,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.001354217529296875,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0016214847564697266,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0015020370483398438,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0015096664428710938,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0016872882843017578,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0022737979888916016,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0019948482513427734,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.001984834671020508,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0020034313201904297,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0023093223571777344,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.002285003662109375,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0023326873779296875,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.002382040023803711,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0026912689208984375,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.002608776092529297,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.00269317626953125,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0029175281524658203,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.003125429153442383,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.003062009811401367,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.003091096878051758,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0035545825958251953,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.003391742706298828,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0034351348876953125,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.003727436065673828,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0038423538208007812,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.004191875457763672,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.004126787185668945,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.004278421401977539,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.004009246826171875,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0043294429779052734,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0043599605560302734,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.004403352737426758,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.004519939422607422,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0046350955963134766,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.004837512969970703,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.004999876022338867,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0049436092376708984,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.005298614501953125,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.005161285400390625,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.006513833999633789,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.005640268325805664,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.005939960479736328,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0056476593017578125,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.005912303924560547,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.005674123764038086,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.005834817886352539,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.005869865417480469,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.006217479705810547,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.008780479431152344,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.006310462951660156,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0065119266510009766,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.006494045257568359,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.006916046142578125,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.006899118423461914,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.006905555725097656,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.007052183151245117,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.006989479064941406,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.007102012634277344,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.007256269454956055,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.007238626480102539,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.007439136505126953,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.007554769515991211,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.011083126068115234,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.007863044738769531,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.007752656936645508,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.00806117057800293,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.008026838302612305,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.01126718521118164,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.008323907852172852,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.008477926254272461,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.008364439010620117,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.008681774139404297,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.008540153503417969,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.00945591926574707,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.009270429611206055,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.008878469467163086,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.009014368057250977,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.009406566619873047,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.00914621353149414,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.009442806243896484,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.009998559951782227,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010426044464111328,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.011830568313598633,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.00989842414855957,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010445833206176758,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010196924209594727,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.01301431655883789,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010573148727416992,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010648488998413086,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010242462158203125,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010707855224609375,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010951519012451172,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010981082916259766,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010902643203735352,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.010851621627807617,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.0012314319610595703,\n",
       " 0.9473684210526315,\n",
       " 0.9069767441860465,\n",
       " -0.0012738704681396484,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0008385181427001953,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0009369850158691406,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0011982917785644531,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0023298263549804688,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.001277923583984375,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0013496875762939453,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0015730857849121094,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582,\n",
       " -0.0017113685607910156,\n",
       " 0.9473684210526315,\n",
       " 0.8837209302325582]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
